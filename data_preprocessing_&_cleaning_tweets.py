# -*- coding: utf-8 -*-
"""Data Preprocessing & Cleaning Tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v3y_5DySjAvqh_Q8hXfbbzAw63YbYQQf

**Data Preprocessing & Cleaning Tweets**

Contents
1.   Lemmatization
2.   Stemmin
3.   Tokenazation
"""

nltk.download('all')

#Download NLTK packages
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from nltk.corpus import stopwords
# 
# nltk.download('stopwords')
# print(stopwords.words('english'))

!pip install contractions

!pip install langdetect

#Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
import string
import re
import contractions

#to detect text language
from langdetect import detect, LangDetectException

#stopwords
from nltk.corpus import stopwords

#Tokenization
from nltk.tokenize import word_tokenize

#Stemming
from nltk.stem import PorterStemmer

#Lemmatizing
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Load csv file containing tweets dataset
def load_data():
    data = pd.read_csv(r"/content/[FULL COMBINED] All Keywords_States (Coordinates) - Snscrape.csv")
    return data

tweet1_df = load_data()
display(tweet1_df)

print('Number of original rows:', len(tweet1_df))

# Load csv file containing tweets dataset
def load_data():
    data = pd.read_csv(r"/content/[FULL COMBINED] All Keywords_Malaysia (Location).csv")
    return data

tweet2_df = load_data()
display(tweet2_df)

print('Number of original rows:', len(tweet2_df))

tweets_df = pd.concat([tweet1_df, tweet2_df], ignore_index=True, sort=False)

tweets_df

#Drop unnecessary columns (ID)
new_df = tweets_df.drop(['ID'], axis=1)
display(new_df)

new_df['Text'][5]

"""**Data Preprocessing**"""

# Initialisation globale pour éviter de recréer à chaque appel
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def expand_contractions(text):
    return contractions.fix(text)

def cleanedTweets(text):
    # Expand contractions
    text = expand_contractions(text)

    # Lowercase
    text = text.lower()

    # Remove mentions, hashtags, numbers, URLs, and punctuation
    text = re.sub(r'(@\w+|#\w+|\d+)', '', text)
    text = re.sub(r'http\S+|www.\S+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'[^\x00-\x7F]+',' ', text)  # Remove non-ASCII chars
    text = re.sub(r'\s+', ' ', text).strip()   # Remove extra spaces

    # Tokenization
    tokens = word_tokenize(text)

    # Stop word removal
    filtered_words = [w for w in tokens if w not in stop_words]

    # Lemmatization
    lemma_words = [lemmatizer.lemmatize(w) for w in filtered_words]

    # Stemming
    stem_words = [stemmer.stem(w) for w in lemma_words]

    # Return cleaned data
    return {
        'Cleaned_Text': " ".join(stem_words),
        'Tokens': tokens,
        'Filtered_Words': filtered_words,
        'Lemmatized_Words': lemma_words,
        'Stemmed_Words': stem_words
    }

# Apply the cleanedTweets function to the 'Text' column and create new columns for each phase
new_df[['Cleaned_Text', 'Tokens', 'Filtered_Words', 'Lemmatized_Words', 'Stemmed_Words']] = new_df['Text'].apply(cleanedTweets).apply(pd.Series)
display(new_df)

new_df['Cleaned_Tweets'] = new_df['Text'].apply(lambda x: cleanedTweets(x))
display(new_df)

import pandas as pd
import ast

def extract_cleaned_text(value):
    if pd.isna(value):
        return ""
    try:
        d = ast.literal_eval(value) if isinstance(value, str) else value
        return d.get("Cleaned_Text", "")
    except Exception:
        return ""

new_df['Cleaned_Text'] = new_df['Cleaned_Tweets'].apply(extract_cleaned_text)

# Preview
print(new_df[['Cleaned_Tweets', 'Cleaned_Text']].head())

new_df.columns

final_df = new_df[['Datetime', 'Username', 'Location', 'Cleaned_Text']].copy()
final_df.head()

final_df.shape

"""**Data Cleaning**"""

final_df = final_df.drop_duplicates(subset='Cleaned_Text')

final_df

import re

def remove_emojis(text):
    if pd.isna(text):
        return ""

    # Emoji pattern
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002700-\U000027BF"  # other symbols
        u"\U0001F900-\U0001F9FF"  # supplemental symbols and pictographs
        u"\U0001FA70-\U0001FAFF"  # symbols and pictographs extended-A
        u"\U000025A0-\U00002BEF"  # geometric shapes and misc symbols
        "]+", flags=re.UNICODE)

    return emoji_pattern.sub(r'', text)

# Apply to the 'Location' column
final_df['Location'] = final_df['Location'].apply(remove_emojis)

# Count duplicates
duplicate_count = final_df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")

final_df.to_csv('CLEANED.csv', index=False)

df = pd.read_csv('/content/CLEANED.csv')

df.shape

from langdetect import detect, DetectorFactory
DetectorFactory.seed = 0  # for consistent results

def detect_language(text):
    try:
        return detect(text)
    except:
        return 'unknown'

# Apply language detection to each tweet
df['Language'] = df['Cleaned_Text'].apply(detect_language)

language_counts = df['Language'].value_counts()

print(language_counts)

df['Detected_Language'] = df['Cleaned_Text'].apply(lambda x: detect(x) if isinstance(x, str) and len(x.split()) > 2 else 'too_short')

lang_counts = df['Detected_Language'].value_counts()

print(lang_counts)

pip install matplotlib

import matplotlib.pyplot as plt

lang_counts = df['Detected_Language'].value_counts()

plt.figure(figsize=(10, 6))
lang_counts.plot(kind='bar', color='skyblue')

plt.title('Distribution of Detected Languages')
plt.xlabel('Language')
plt.ylabel('Number of Entries')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

filtered_counts = lang_counts.drop(['too_short'], errors='ignore')

filtered_counts

df

df.columns

# Filter rows with selected languages
languages_to_keep = ['id', 'en', 'tl']
filtered_df = df[df['Detected_Language'].isin(languages_to_keep)]

# Get distribution
language_distribution = filtered_df['Detected_Language'].value_counts().reset_index()
language_distribution.columns = ['language', 'count']

# Calculate percentage
total = language_distribution['count'].sum()
language_distribution['percentage'] = (language_distribution['count'] / total) * 100

print(language_distribution)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(data=language_distribution, x='language', y='count')
plt.title('Distribution of Kept Languages')
plt.xticks(rotation=45)
plt.show()

filtered_df

df_indonesian = filtered_df[df['Detected_Language'] == 'id'].copy()
df_english = filtered_df[df['Detected_Language'] == 'en'].copy()
df_tagalog = filtered_df[df['Detected_Language'] == 'tl'].copy()

df_indonesian

df_indonesian.to_csv('indon.csv', index=False)

pip install transformers sentencepiece

!pip install hf_xet

"""**⚡️ Optimized Script for Batched Indonesian → English Translation on GPU**"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
import torch

# === CONFIG ===
csv_path = "/content/indon.csv"                  # Your CSV file
text_column = "Cleaned_Text"       # Column with Indonesian sentences
output_column = "Cleaned_Text_translation" # New column name
output_csv_path = "indo_translated_data.csv"
batch_size = 32                        # Batch size (tune as needed)

# === Load model and tokenizer ===
model_name = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# === Use GPU if available ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# === Language codes ===
src_lang = "ind_Latn"
tgt_lang = "eng_Latn"
tokenizer.src_lang = src_lang

# === Load dataset ===
df = pd.read_csv(csv_path)
texts = df[text_column].fillna("").astype(str).tolist()

# === Batched translation ===
def translate_batch(text_list):
    inputs = tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=256)
    inputs = {key: val.to(device) for key, val in inputs.items()}
    outputs = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),
        max_length=256
    )
    return [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]

# === Apply batching ===
translated = []
for i in tqdm(range(0, len(texts), batch_size)):
    batch = texts[i:i + batch_size]
    translated_batch = translate_batch(batch)
    translated.extend(translated_batch)

# === Save results ===
df[output_column] = translated
df.to_csv(output_csv_path, index=False)

print(f"✅ Translation complete! Saved to '{output_csv_path}'")

td_df= pd.read_csv('/content/indo_translated_data.csv')

td_df

td_df.columns

new_df = td_df[['Datetime','Username', 'Location', 'Cleaned_Text_translation']].copy()
new_df

new_df.to_csv('indon_transla.csv', index=False)

df_english.to_csv('eng.csv', index=False)

df_tagalog.to_csv('tagalog.csv', index=False)

df_tagalog

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
import torch

# === CONFIG ===
csv_path = "/content/tagalog.csv"                  # Your CSV file
text_column = "Cleaned_Text"       # Column with Indonesian sentences
output_column = "Cleaned_Text_translation" # New column name
output_csv_path = "talog_translated_data.csv"
batch_size = 32                        # Batch size (tune as needed)

# === Load model and tokenizer ===
model_name = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# === Use GPU if available ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# === Language codes ===
src_lang = "ind_Latn"
tgt_lang = "eng_Latn"
tokenizer.src_lang = src_lang

# === Load dataset ===
df = pd.read_csv(csv_path)
texts = df[text_column].fillna("").astype(str).tolist()

# === Batched translation ===
def translate_batch(text_list):
    inputs = tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=256)
    inputs = {key: val.to(device) for key, val in inputs.items()}
    outputs = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),
        max_length=256
    )
    return [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]

# === Apply batching ===
translated = []
for i in tqdm(range(0, len(texts), batch_size)):
    batch = texts[i:i + batch_size]
    translated_batch = translate_batch(batch)
    translated.extend(translated_batch)

# === Save results ===
df[output_column] = translated
df.to_csv(output_csv_path, index=False)

print(f"✅ Translation complete! Saved to '{output_csv_path}'")

tl_df= pd.read_csv('/content/talog_translated_data.csv')

tl_df

neww_df = tl_df[['Datetime','Username', 'Location', 'Cleaned_Text_translation']].copy()
neww_df

df_english

eng_df = df_english[['Datetime','Username', 'Location', 'Cleaned_Text']].copy()
eng_df

neww_df.to_csv('tl_transla.csv', index=False)

new_df = new_df.rename(columns={'Cleaned_Text_translation': 'Cleaned_Text'})
neww_df = new_df.rename(columns={'Cleaned_Text_translation': 'Cleaned_Text'})

new_df = new_df.rename(columns={'Cleaned_text': 'Cleaned_Text'})
neww_df = new_df.rename(columns={'Cleaned_text': 'Cleaned_Text'})

eng_df

new_df

neww_df

df = pd.concat([new_df, neww_df, eng_df], axis=0, ignore_index=True)

df

df.to_csv('Full_combinaed_english_cleaned_data.csv', index=False)