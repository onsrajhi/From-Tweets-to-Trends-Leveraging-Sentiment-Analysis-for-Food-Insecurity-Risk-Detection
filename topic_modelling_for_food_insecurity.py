# -*- coding: utf-8 -*-
"""Topic Modelling for Food Insecurity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10B_GC4LOEFswmV8B8Kt_BbmNIDiSK_Ts
"""

!pip install pyLDAvis

!pip install --upgrade numpy pandas

!pip install --upgrade --force-reinstall numpy
!pip install --upgrade pandas

!pip uninstall numpy
!pip install numpy

import os
import sys
os.kill(os.getpid(), 9)

!pip uninstall -y numpy pandas gensim
!pip install numpy==1.24.4 pandas==1.5.3 gensim==4.3.1

#import libraries
import pandas as pd
import numpy as np
import gensim
from gensim import corpora, models, similarities
from gensim.models import LdaModel, CoherenceModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from string import punctuation
from collections import OrderedDict
import seaborn as sns
# import pyLDAvis.gensim
import matplotlib.pyplot as plt

# Nettoyage complet
!pip uninstall -y numpy scipy gensim pandas

# R√©installation avec versions compatibles
!pip install numpy==1.24.4 scipy==1.10.1 pandas==1.5.3 gensim==4.3.1

import pandas as pd
import numpy as np
import gensim
from gensim import corpora, models, similarities
from gensim.models import LdaModel, CoherenceModel

#import libraries
import pandas as pd
import numpy as np
import gensim
from gensim import corpora, models, similarities
from gensim.models import LdaModel, CoherenceModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from string import punctuation
from collections import OrderedDict
import seaborn as sns
# import pyLDAvis.gensim
import matplotlib.pyplot as plt

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

nltk.download('all')

# Load the CLEANED tweets dataset (w/ Sentiment)
tweets_dataset = pd.read_csv(r"/content/V3 Harmonized VADER_TextBlob.csv")
display(tweets_dataset, len(tweets_dataset))

from nltk.tokenize import word_tokenize

# Remove rows with missing values in 'Cleaned_Tweets' column
tokenized_tweets = tweets_dataset.dropna(subset=['Cleaned_Tweets'])

# Tokenize the tweets
tokenized_tweets['Tokenized_Tweets'] = tokenized_tweets['Cleaned_Tweets'].apply(lambda x: word_tokenize(str(x)))

# Display the 'Cleaned_Tweets' and 'Tokenized_Tweets' columns
display(tokenized_tweets[['Cleaned_Tweets', 'Tokenized_Tweets']])

# Convert the 'Cleaned_Tweets' column to strings
tweets_dataset['Cleaned_Tweets'] = tweets_dataset['Cleaned_Tweets'].astype(str)

# Tokenize the cleaned tweets and store them in a new column 'Tokenized_Tweets'
tweets_dataset['Tokenized_Tweets'] = tweets_dataset['Cleaned_Tweets'].apply(word_tokenize)
display(tweets_dataset)

import re
import string
import spacy
from nltk.corpus import stopwords

nlp = spacy.load("en_core_web_sm")

from nltk.corpus import stopwords

# Stopwords de base
stop_words = set(stopwords.words('english'))

# Stopwords personnalis√©s √©tendus
custom_stopwords = {
    "lol", "btw", "idk", "u", "omg", "tbh", "afaik", "smh", "fyi",
    "ikr", "lmao", "rofl", "wtf", "thx", "pls", "imo", "imho",
    "cuz", "ya", "yeah", "yep", "nah", "nope", "hmm", "eh", "ugh",
    "brb", "gtg", "ttyl", "bbl", "nvm", "irl", "jk", "tho", "cya",
    "luv", "yo", "hey", "hi", "hello", "ok", "okay", "right", "thanks",
    "thank", "please"
}

# Fusion avec les stopwords de base
stop_words = stop_words.union(custom_stopwords)

normalization_dict = {
    "hungri": "hungry",
    "realli": "really",
    "u": "you",
    "gonna": "going to",
    "wanna": "want to",
    "luv": "love",
    "pls": "please",
    "thx": "thanks",
    "kinda": "kind of",
    "gotta": "got to",
    "ain't": "is not",
    "cuz": "because",
    "bc": "because",
    "idk": "I don't know",
    "imma": "I am going to",
    "lemme": "let me",
    "dunno": "don't know",
    "gimme": "give me",
    "ya": "you",
    "tho": "though",
    "brb": "be right back",
    "btw": "by the way",
    "sooo": "so",
    "yeah": "yes",
    "yep": "yes",
    "nah": "no",
    "nope": "no",
    "hmm": "hmm",
    "eh": "eh",
    "ugh": "ugh",
    "lol": "laugh out loud",
    "omg": "oh my god",
    "tbh": "to be honest",
    "afaik": "as far as I know",
    "smh": "shaking my head",
    "fyi": "for your information",
    "ikr": "I know right",
    "lmao": "laugh my ass off",
    "rofl": "rolling on the floor laughing",
    "wtf": "what the fuck",
    "imo": "in my opinion",
    "imho": "in my humble opinion",
    "c'mon": "come on",
    "fomo": "fear of missing out",
    "bday": "birthday",
    "afk": "away from keyboard",
    "ftw": "for the win",
    "idc": "I don't care",
    "nvm": "never mind",
    "yw": "you're welcome",
    "np": "no problem",
    "wtv": "whatever",
    "tbqh": "to be quite honest",
    "tbf": "to be fair",
    "fwiw": "for what it's worth",
}

def normalize_text(text):
    # Tokenisation simple
    tokens = text.split()
    normalized_tokens = []
    for token in tokens:
        # conversion en minuscule + normalisation si dans dict
        token_lower = token.lower()
        if token_lower in normalization_dict:
            normalized_tokens.append(normalization_dict[token_lower])
        else:
            normalized_tokens.append(token_lower)
    return " ".join(normalized_tokens)

def preprocess_text(text):
    # 1) Normalisation orthographique
    text = normalize_text(text)

    # 2) Suppression ponctuation (remplacer par espace pour ne pas coller mots)
    text = re.sub(f"[{re.escape(string.punctuation)}]", " ", text)

    # 3) Traitement spacy pour tokenisation + lemmatisation
    doc = nlp(text)

    lemmas = []
    for token in doc:
        # 4) Stopwords filtering + pas de ponctuation + pas d'espaces
        if token.lemma_ not in stop_words and token.lemma_.strip() != "" and not token.is_punct:
            lemmas.append(token.lemma_)

    # Reconstituer le texte nettoy√©
    return " ".join(lemmas)

!pip install pyspellchecker nltk spacy
!python -m nltk.downloader stopwords
!python -m spacy download en_core_web_sm

import string
from spellchecker import SpellChecker
import spacy
from nltk.corpus import stopwords

# Chargement des ressources
spell = SpellChecker()
nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

normalization_dict = {
    "hungri": "hungry",
    "realli": "really",
    "u": "you",
    "gonna": "going to",
    "wanna": "want to",
    "luv": "love",
    "pls": "please",
    "thx": "thanks",
    "kinda": "kind of",
    "gotta": "got to",
    "ain't": "is not",
    "cuz": "because",
    "bc": "because",
    "idk": "I don't know",
    "imma": "I am going to",
    "lemme": "let me",
    "dunno": "don't know",
    "gimme": "give me",
    "ya": "you",
    "tho": "though",
    "brb": "be right back",
    "btw": "by the way",
    "sooo": "so",
    "yeah": "yes",
    "yep": "yes",
    "nah": "no",
    "nope": "no",
    "hmm": "hmm",
    "eh": "eh",
    "ugh": "ugh",
    "lol": "laugh out loud",
    "omg": "oh my god",
    "tbh": "to be honest",
    "afaik": "as far as I know",
    "smh": "shaking my head",
    "fyi": "for your information",
    "ikr": "I know right",
    "lmao": "laugh my ass off",
    "rofl": "rolling on the floor laughing",
    "wtf": "what the fuck",
    "imo": "in my opinion",
    "imho": "in my humble opinion",
    "c'mon": "come on",
    "fomo": "fear of missing out",
    "bday": "birthday",
    "afk": "away from keyboard",
    "ftw": "for the win",
    "idc": "I don't care",
    "nvm": "never mind",
    "yw": "you're welcome",
    "np": "no problem",
    "wtv": "whatever",
    "tbqh": "to be quite honest",
    "tbf": "to be fair",
    "fwiw": "for what it's worth",
}

def normalize_and_correct(token):
    token = token.lower().strip(string.punctuation)

    # 1. V√©rifie s‚Äôil est dans le dictionnaire de normalisation
    if token in normalization_dict:
        return normalization_dict[token]

    # 2. Sinon, tente une correction orthographique
    corrected = spell.correction(token)

    return corrected if corrected else token

def preprocess_text(text):
    # 1. Tokenisation basique par espace
    raw_tokens = text.split()

    # 2. Nettoyage et normalisation + correction orthographique
    normalized_tokens = [normalize_and_correct(token) for token in raw_tokens]

    # 3. Recompose le texte nettoy√© pour spaCy
    cleaned_text = " ".join(normalized_tokens)

    # 4. Traitement spaCy
    doc = nlp(cleaned_text)

    # 5. Lemmatisation + suppression stopwords + ponctuation
    final_tokens = [
        token.lemma_ for token in doc
        if token.lemma_ not in stop_words and not token.is_punct and token.lemma_.strip() != ""
    ]

    return " ".join(final_tokens)

# üîç Exemple
examples = [
    "I sooo hungri I realli want eat help I",
    "Yeah, I‚Äôm worri about the food price.",
    "Gimme food pls I'm sooo tired",
    "idk what to do, u kno what i mean?"
]

for text in examples:
    print("ORIGINAL:", text)
    print("CLEANED :", preprocess_text(text))
    print("‚Äî" * 50)

# Exemple
text = "I am sooo hungri and I realli wanna eat! Can u pls help me? LOL."
clean_text = preprocess_text(text)
print(clean_text)





"""**Create Dictionary and Tweet Corpus**"""

from gensim import corpora

# Create a dictionary from the tokenized tweets
dictionary = corpora.Dictionary(tweets_dataset['Tokenized_Tweets'])

# Create a corpus
tweet_corpus = [dictionary.doc2bow(tweet) for tweet in tweets_dataset['Tokenized_Tweets']]

#display tweet corpus
display(tweet_corpus)

"""**Define No. of Topics to Try to Build LDA Model & Compute Coherence Score**"""

from gensim.models import CoherenceModel

# Define the number of topics you want to try (4 in this case)
num_topics_list = [4]

coherence_scores = []
for num_topics in num_topics_list:
    # Train the LDA model
    lda_model = models.LdaModel(corpus=tweet_corpus, id2word=dictionary, num_topics=num_topics, passes=10)

    # Compute coherence score
    coherence_model = CoherenceModel(model=lda_model, texts=tweets_dataset['Tokenized_Tweets'], dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    coherence_scores.append((num_topics, coherence_score))

# Find the number of topics with the highest coherence score
best_num_topics = max(coherence_scores, key=lambda x: x[1])[0]
print(f"The optimal number of topics is: {best_num_topics}")

"""**Train the Final LDA model with Best No. of Topics**"""

# Train the final LDA model with the best number of topics
lda_model = models.LdaModel(corpus=tweet_corpus, id2word=dictionary, num_topics=best_num_topics, passes=10)

# Assign topics to each tweet in the dataset
topics_assigned = []
for tweet_bow in tweet_corpus:
    topic_distribution = lda_model.get_document_topics(tweet_bow)
    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]
    topics_assigned.append(dominant_topic)

# Add the assigned topics to the tweets dataset
tweets_dataset['Assigned_Topic'] = topics_assigned

# Map the topic number to the corresponding topic name
topic_names = {
    0: 'Not enough food',
    1: 'Unaffordability',
    2: 'Difficulty in finding food',
    3: 'Food insecurity'
}
tweets_dataset['Topic_Name'] = tweets_dataset['Assigned_Topic'].map(topic_names)
display(tweets_dataset)

lda_model.print_topics()

lda_model.show_topics()

# Count the number of tweets for each topic
topic_counts = tweets_dataset['Topic_Name'].value_counts().reset_index()
topic_counts.columns = ['Topic_Name', 'Tweet_Count']

# Display the number of tweets per topic
print("Number of tweets per topic:")
display(topic_counts)

import matplotlib.pyplot as plt

# Count the number of tweets for each topic
topic_counts = tweets_dataset['Topic_Name'].value_counts().reset_index()
topic_counts.columns = ['Topic_Name', 'Tweet_Count']

# Display the number of tweets per topic
print("Number of tweets per topic:")
display(topic_counts)

# Plot a pie chart for the tweet counts
plt.figure(figsize=(8, 6))
plt.pie(
    topic_counts['Tweet_Count'],
    labels=topic_counts['Topic_Name'],
    autopct=lambda pct: f"{pct:.1f}%\n({int(pct/100*sum(topic_counts['Tweet_Count']))})",
    startangle=90
)
plt.title("Number of Tweets per Topic")
plt.axis('equal')

# Show the pie chart
plt.show()

#Visualize topic distribution
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Convert the gensim LDA model to a format compatible with pyLDAvis
vis_data = gensimvis.prepare(lda_model, tweet_corpus, dictionary)

# Display the visualization
pyLDAvis.display(vis_data)

# Print the top 30 words for each topic
num_words = 30
for topic_id in range(best_num_topics):
    topic_words = lda_model.show_topic(topic_id, topn=num_words)
    topic_words = [word for word, _ in topic_words]
    print(f"Topic {topic_id + 1}:")
    print(', '.join(topic_words))
    print()

"""**Create word cloud for each topic**"""

from wordcloud import WordCloud

# Create word clouds for each topic
for topic_id in topic_names.keys():
    # Get the tweets corresponding to the topic
    topic_tweets = tweets_dataset[tweets_dataset['Assigned_Topic'] == topic_id]['Tokenized_Tweets']
    # Concatenate the tokens into a single string
    topic_words = ' '.join(topic_tweets.sum())
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(topic_words)
    # Plot the word cloud
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Topic: {topic_names[topic_id]}', fontsize=14)
    plt.axis('off')
    plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Define the color map for each topic
color_map = {
    0: 'Reds',
    1: 'Blues',
    2: 'Greens',
    3: 'Wistia'
}

# Create subplots for each topic
fig, axs = plt.subplots(1, best_num_topics, figsize=(12, 6))

# Generate word clouds for each topic
for topic_id in range(best_num_topics):
    # Filter the tweets for the current topic
    topic_tweets = tweets_dataset[tweets_dataset['Assigned_Topic'] == topic_id]

    # Concatenate all tokenized tweets for the current topic
    topic_tokens = ' '.join([token for tweet_tokens in topic_tweets['Tokenized_Tweets'] for token in tweet_tokens])

    # Generate a word cloud for the current topic
    wordcloud = WordCloud(background_color='white', colormap=color_map[topic_id]).generate(topic_tokens)

    # Plot the word cloud in the corresponding subplot
    axs[topic_id].imshow(wordcloud, interpolation='bilinear')
    axs[topic_id].set_title(f"Topic {topic_id + 1} Word Cloud")
    axs[topic_id].axis('off')

# Adjust spacing between subplots
plt.subplots_adjust(wspace=0.2)

# Show the subplots
plt.show()

#Save the VADER sentiment dataset
tweets_dataset.to_csv('Topic Modelling.csv', index=False)

"""**Extracting Top 20 Keywords from the Dataset**"""

import pandas as pd
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Load csv file containing tweets dataset (w/ sentiments)

tweets_df = pd.read_csv(r"/content/V3 Harmonized VADER_TextBlob.csv")
display(tweets_df)

# Assuming df is your DataFrame and 'Cleaned Tweets' is your column of interest
words = ' '.join(tweets_df['Cleaned_Tweets']).lower().split()
words = [word for word in words if word not in stopwords.words('english')]

# Count the frequency of each word
word_freq = Counter(words)

# Create a DataFrame from the dictionary
word_freq_df = pd.DataFrame.from_dict(word_freq, orient='index').reset_index()
word_freq_df.columns = ['Keyword', 'Frequency']

# Get top 20 words
top_20_words = word_freq_df.sort_values(by='Frequency', ascending=False).head(20)

display(top_20_words)

#Save results into CSV file
top_20_words.to_csv('Top 20 Keywords FI Risk.csv', index=False)

import pandas as pd
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Load csv file containing tweets dataset (w/ sentiments)

tweets_df = pd.read_csv(r"/content/V3 Harmonized VADER_TextBlob.csv")
display(tweets_df)

# Assuming df is your DataFrame and 'Cleaned Tweets' is your column of interest
words = ' '.join(tweets_df['Cleaned_Tweets']).lower().split()
words = [word for word in words if word not in stopwords.words('english')]

# Count the frequency of each word
word_freq = Counter(words)

# Create a DataFrame from the dictionary
word_freq_df = pd.DataFrame.from_dict(word_freq, orient='index').reset_index()
word_freq_df.columns = ['Keyword', 'Frequency']

# Get top 20 words
top_20_words = word_freq_df.sort_values(by='Frequency', ascending=False).head(20)

display(top_20_words)

#Save results into CSV file
top_20_words.to_csv('Top 20 Keywords SA FI Risk.csv', index=False)

coherence_model_lda = CoherenceModel(model=lda_model, texts=tweets_dataset['Tokenized_Tweets'], dictionary=dictionary, coherence='c_v')
coherence_score = coherence_model_lda.get_coherence()
print(f"Coherence Score: {coherence_score}")

import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, tweet_corpus, dictionary)
pyLDAvis.display(vis)

